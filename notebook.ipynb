{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "### TODO\n",
    "#### We are using a CNN, max pooling, and n-grams (a sequence of n words in a sentence) to construct this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imporant Documentation\n",
    "\n",
    "### PyTorch Resources:\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "#####    - https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058#:~:text=An%20N%2Dgram%20means%20a,3%2Dgram%20(trigram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tokenize data and build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc8d35b5542b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, vocab_size, seq_len):\n",
    "        self.file_name = 'clickbait_data.csv'\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_padded = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_dataset(self):\n",
    "        # Load dataset from local directory \n",
    "        df = pd.read_csv(self.file_name)\n",
    "        \n",
    "        self.x = df['headline'].values\n",
    "        self.y = df['clickbait'].values\n",
    "\n",
    "    def clean_data(self):\n",
    "        # Clean data by removing all special characters. Convert words to lowercase\n",
    "        self.x = [re.sub(r'\\'','', headline).lower() for headline in self.x]\n",
    "        self.x = [re.sub(r'[^A-Za-z0-9]+',' ', headline).lower() for headline in self.x]\n",
    "\n",
    "    def tokenization(self):\n",
    "        # Tokenize all headlines\n",
    "        self.x = [nltk.tokenize.wordpunct_tokenize(headline) for headline in self.x]\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # Build vocab and return 'vocab_size' most common words\n",
    "        self.vocab = dict()\n",
    "\n",
    "        fdist = nltk.FreqDist()\n",
    "        for headline in self.x:\n",
    "            for word in headline:\n",
    "                fdist[word] += 1\n",
    "        \n",
    "        common_words = fdist.most_common(self.vocab_size)\n",
    "\n",
    "        for count, word in enumerate(common_words):\n",
    "            self.vocab[word[0]] = count+1\n",
    "    \n",
    "    def word_to_idx(self):\t\n",
    "        # Convert each token into index based representation \n",
    "        self.x_tokenized = list()\n",
    "        \n",
    "        for sentence in self.x:\n",
    "            temp = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocab.keys():\n",
    "                    temp.append(self.vocab[word])\n",
    "            self.x_tokenized.append(temp)\n",
    "    \n",
    "    def padding_sentences(self):\n",
    "        # Make all sentences equal length. \n",
    "        # If sentence is smaller than minimum length, pad it \n",
    "        idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), idx)\n",
    "            self.x_padded.append(sentence)\n",
    "            \n",
    "        self.x_padded = np.array(self.x_padded)\n",
    "\n",
    "    def split_data(self):\n",
    "        # Split data into training and testing sets\n",
    "        trnSize = int(len(self.x) * .8)\n",
    "        tstSize = int(len(self.x) * .2)\n",
    "        self.x_train = self.x[:trnSize]\n",
    "        self.y_train = self.y[:trnSize]\n",
    "        self.x_test = self.x[trnSize:]\n",
    "        self.y_test = self.y[trnSize:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadlineClassifier(torch.nn.ModuleList):\n",
    "    def __init__(self, seq_len, num_words, embedding_size, dropout, out_size, stride, filters):\n",
    "        super(HeadlineClassifier, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.num_words = num_words\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Dropout (used to reduce chance of overfitting by \"dropping\" units in neural net). Probability (p) set to 0.25\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Kernel sizes for CNN\n",
    "        #pass it as params when we change the implmentation. Can keep it like this for now\n",
    "        self.filters = filters\n",
    "\n",
    "        # Output size for convolutions\n",
    "        self.out_size = out_size\n",
    "        # Number of strides for convolutions\n",
    "        self.stride = stride\n",
    "\n",
    "        # Embedding layer (lookup table that stores embeddings of a fixed dictionary and size)\n",
    "        self.embedding = torch.nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "\n",
    "        # Convolution layers (each is a 1D convolution over an input)\n",
    "        self.clayers = [torch.nn.Conv1d(self.seq_len, self.out_size, fltr, self.stride) for fltr in self.filters]\n",
    "        \n",
    "            \n",
    "        # Max pooling layers (each applies 1D max pooling to input) \n",
    "        self.poollayers = [torch.nn.MaxPool1d(fltr, self.stride) for fltr in self.filters]\n",
    "\n",
    "        # Fully connected layer (applies linear transformation to data)\n",
    "        self.fc = torch.nn.Linear(self.size_of_input(), 1)\n",
    "\n",
    "    \n",
    "    def size_of_input(self):\n",
    "\n",
    "        pout = 0\n",
    "        for filter in self.filters:\n",
    "            cout = math.floor(((self.embedding_size + (2 * self.padding) - self.dilation * (filter - 1) - 1) / self.stride) + 1)\n",
    "            pout += math.floor(((cout - (filter - 1) - 1) / self.stride) + 1)\n",
    "        \n",
    "        return pout * self.out_size\n",
    "\n",
    "    '''Create forward pass of neural network. Consists of mainly ordering the different types of layers'''\n",
    "    def forward_pass(self, input_X):\n",
    "        \n",
    "        x = self.embedding(input_X)\n",
    "\n",
    "        layerPasses = list()\n",
    "        for i in range(len(self.filters)):\n",
    "            temp = self.clayers[i](x)\n",
    "            #Why not sigmoid?\n",
    "            temp = torch.relu(temp)\n",
    "            temp = self.poollayers[i](temp)\n",
    "            layerPasses.append(temp)\n",
    "\n",
    "        unn = torch.cat(layerPasses, 2)\n",
    "        unn = unn.reshape(unn.size(0), -1)\n",
    "\n",
    "        output = torch.sigmoid(self.dropout(self.fc(unn))).squeeze()\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass(torch.utils.data.Dataset):\n",
    "   def __init__(self, x, y):\n",
    "      self.x = x\n",
    "      self.y = y\n",
    "      \n",
    "   def __len__(self):\n",
    "      return len(self.x)\n",
    "      \n",
    "   def __getitem__(self, idx):\n",
    "      return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, learning_rate, batch_size, train_data, test_data, numOfIter):\n",
    "    train_loader = torch.utils.data.Dataloader(train_data, batch_size)\n",
    "    test_loader = torch.utils.data.Dataloader(test_data, batch_size)\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    for iter in numOfIter:\n",
    "        model.train()\n",
    "        train_predictions = []\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            y = y.type(torch.FloatTensor)\n",
    "            \n",
    "            pred = model(x)\n",
    "\n",
    "            loss = torch.nn.functional.binary_cross_entropy(pred, y)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            loss.backwards()\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            train_predictions += list(pred.detach().numpy())\n",
    "\n",
    "        # Evaluate model\n",
    "        model.eval()\n",
    "        test_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                pred = model(x)\n",
    "                test_predictions += list(y.detach().numpy())\n",
    "\n",
    "        # Get accuracy \n",
    "        train_acc = calc_acc(dataset['y_train'], train_predictions)\n",
    "        test_acc = calc_acc(dataset['y_test'], test_predictions)\n",
    "                \n",
    "        print(\"Iteration: %d, Loss: %.5f, Train Accuracy: %.5f\" % (iter+1, loss.item(), train_acc))\n",
    "        print(\"Iteration: %d, Loss: %.5f, Test Accuracy: %.5f\" % (iter+1, loss.item(), test_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(actual, predictions):\n",
    "    clickbait = 0\n",
    "    not_clickbait = 0\n",
    "\n",
    "    for true, pred in zip(actual, predictions):\n",
    "        if (pred >= .5) and (true == 1):\n",
    "            clickbait += 1\n",
    "        elif (pred < .5) and (true == 0):\n",
    "            not_clickbait += 1\n",
    "\n",
    "    return (clickbait + not_clickbait) / len(actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  32000\n",
      "Dataset vocab:  {'to': 1, 'in': 2, 'the': 3, 'of': 4, 'you': 5, 'a': 6, 'for': 7, 'and': 8, 'on': 9, 'your': 10, 'is': 11, 'are': 12, 'that': 13, 'this': 14, 'with': 15, 'at': 16, 'will': 17, 'from': 18, 'new': 19, 'about': 20, 'what': 21, 'who': 22, 'people': 23, 'things': 24, 'how': 25, 'which': 26, 'us': 27, 'as': 28, 'can': 29, 'by': 30, 'make': 31, 'we': 32, 'know': 33, 'be': 34, 'after': 35, '17': 36, 'do': 37, '21': 38, 's': 39, 'u': 40, 'should': 41, 'have': 42, 'these': 43, 'based': 44, 'all': 45, '19': 46, 'actually': 47, 'up': 48, 'it': 49, 'over': 50, 'their': 51, 'times': 52, 'an': 53, 'out': 54, 'first': 55, 'was': 56, 'its': 57, 'like': 58, 'if': 59, 'one': 60, '2015': 61, 'most': 62, 'or': 63, 'best': 64, 'more': 65, 'life': 66, 'when': 67, 'need': 68, 'heres': 69, 'has': 70, 'time': 71, 'world': 72, 'his': 73, 'just': 74, 'i': 75, '15': 76, 'dead': 77, '23': 78, '18': 79, 'year': 80, 'day': 81, 'her': 82, 'get': 83, 'killed': 84, 'dies': 85, 'ever': 86, 'every': 87, 'two': 88, 'president': 89, 'real': 90, 'not': 91, 'were': 92, 'into': 93, 'love': 94, 'says': 95, 'youre': 96, 'too': 97, 'uk': 98, 'zodiac': 99, 'no': 100, 'british': 101, '22': 102, 'everyone': 103, '16': 104, 'man': 105, 'favorite': 106, 'only': 107, 'australian': 108, 'kills': 109, 'way': 110, 'ways': 111, 'years': 112, 'photos': 113, 'would': 114, 'wins': 115, 'game': 116, 'sign': 117, '13': 118, 'now': 119, 'star': 120, 'show': 121, 'women': 122, 'well': 123, 'obama': 124, 'pictures': 125, 'tweets': 126, 'off': 127, 'police': 128, 'christmas': 129, 'understand': 130, 'character': 131, 'youll': 132, '24': 133, 'halloween': 134, 'had': 135, 'really': 136, 'video': 137, 'reasons': 138, '11': 139, 'but': 140, 'china': 141, 'guess': 142, 'movie': 143, 'court': 144, 'never': 145, 'so': 146, 'look': 147, 'found': 148, 'questions': 149, 'sex': 150, 'iraq': 151, '27': 152, 'disney': 153, 'may': 154, 'old': 155, 'why': 156, 'try': 157, 'than': 158, 'being': 159, '12': 160, '2': 161, 'tv': 162, 'tell': 163, '25': 164, '7': 165, 'down': 166, '10': 167, 'made': 168, 'say': 169, 'want': 170, 'harry': 171, 'former': 172, 'test': 173, 'they': 174, 'whats': 175, 'woman': 176, 'home': 177, 'against': 178, 'south': 179, '14': 180, 'crash': 181, '9': 182, 'hilarious': 183, 'food': 184, 'see': 185, 'government': 186, 'week': 187, 'age': 188, '1': 189, 'house': 190, 'american': 191, 'thing': 192, 'looks': 193, 'take': 194, 'watch': 195, 'canadian': 196, 'pakistan': 197, 'school': 198, 'three': 199, 'party': 200, 'fire': 201, '2016': 202, 'north': 203, '5': 204, 'right': 205, 'football': 206, 'girls': 207, 'remember': 208, 'kids': 209, 'song': 210, 'back': 211, 'least': 212, 'case': 213, 'got': 214, '20': 215, 'potter': 216, 'perfect': 217, 'girl': 218, 'win': 219, '2008': 220, 'death': 221, 'instagram': 222, 'bomb': 223, '3': 224, 'internet': 225, 'laugh': 226, 'some': 227, 'india': 228, 'much': 229, 'york': 230, 'london': 231, 'state': 232, 'during': 233, '26': 234, 'black': 235, 'fall': 236, 'arrested': 237, 'give': 238, 'gifts': 239, 'big': 240, 'hair': 241, 'name': 242, 'adorable': 243, 'anyone': 244, 'totally': 245, 'men': 246, 'city': 247, '8': 248, 'music': 249, 'movies': 250, 'air': 251, 'little': 252, 'election': 253, 'help': 254, 'said': 255, '29': 256, 'friends': 257, 'thatll': 258, 'before': 259, 'go': 260, 'confessions': 261, 'united': 262, 'california': 263, 'leader': 264, 'dog': 265, 'perfectly': 266, 'million': 267, 'wars': 268, 'better': 269, 'n': 270, 'minister': 271, 'youve': 272, 'attack': 273, 'celebrity': 274, 'four': 275, 'prove': 276, 'high': 277, 'cup': 278, 'group': 279, 'my': 280, 'could': 281, 'find': 282, 'last': 283, '6': 284, 'iran': 285, 'been': 286, 'worst': 287, 'canada': 288, 'west': 289, '4': 290, 'makeup': 291, '000': 292, 'war': 293, 'family': 294, 'while': 295, 'england': 296, 'bank': 297, 'signs': 298, 'work': 299, 'taylor': 300, 'afghanistan': 301, 'wedding': 302, 'fans': 303, 'good': 304, 'military': 305, 'still': 306, 'many': 307, 'deal': 308, 'australia': 309, 'guy': 310, 'top': 311, 'question': 312, 'plan': 313, 'earthquake': 314, 'baby': 315, 'happened': 316, 'vs': 317, '31': 318, 'talk': 319, 'france': 320, 'true': 321, 'live': 322, 'everything': 323, 'states': 324, 'makes': 325, 'league': 326, 'here': 327, 'car': 328, 'second': 329, 'report': 330, 'wikinews': 331, 'indian': 332, 'amazing': 333, 'season': 334, 'plane': 335, 'characters': 336, 'dont': 337, 'feel': 338, 'through': 339, 'presidential': 340, 'hot': 341, 'person': 342, 'moments': 343, 'suicide': 344, 'g': 345, '100': 346, 'change': 347, 'easy': 348, 'delicious': 349, 'book': 350, 'bush': 351, 'face': 352, 'struggles': 353, 'college': 354, 'oil': 355, '30': 356, 'plans': 357, 'kill': 358, 'c': 359, 'rules': 360, 'health': 361, 'set': 362, 'korea': 363, 'did': 364, 'thanksgiving': 365, 'going': 366, 'use': 367, 'single': 368, 'red': 369, 'he': 370, 'announces': 371, 'stop': 372, 'die': 373, 'other': 374, 'af': 375, 'eat': 376, 'space': 377, 'obsessed': 378, 'russian': 379, 'claims': 380, 'shows': 381, 'again': 382, 'french': 383, '2007': 384, 'does': 385, 'white': 386, 'dogs': 387, 'five': 388, 'final': 389, 'without': 390, 'games': 391, 'hits': 392, 'products': 393, 'twitter': 394, 'swift': 395, 'record': 396, 'vote': 397, 'bill': 398, 'aid': 399, 'cat': 400, 'm': 401, 'chinese': 402, 'she': 403, 'words': 404, 'faces': 405, 'same': 406, 'open': 407, 'office': 408, 'hit': 409, 'cast': 410, 'takes': 411, 'un': 412, 'russia': 413, 'iraqi': 414, 'guys': 415, 'tried': 416, 'zealand': 417, 'next': 418, 'went': 419, 'super': 420, 'shot': 421, 'nuclear': 422, 'force': 423, 'murder': 424, 'needs': 425, 'news': 426, 'photo': 427, 'beautiful': 428, 'coast': 429, 'six': 430, 'texas': 431, 'billion': 432, 'books': 433, 'injured': 434, 'grand': 435, 'elections': 436, 'launches': 437, 'friend': 438, 'long': 439, 'gay': 440, 'series': 441, 'near': 442, 'chief': 443, 'florida': 444, 'talks': 445, 'tumblr': 446, 'ban': 447, 'international': 448, 'trial': 449, 'someone': 450, 'john': 451, 'calls': 452, 'own': 453, 'our': 454, 'guaranteed': 455, 'relationship': 456, 'flight': 457, 'european': 458, 'afghan': 459, 'crashes': 460, 'making': 461, 'cute': 462, 'around': 463, 'date': 464, 'lyrics': 465, '90s': 466, 'parents': 467, 'candidate': 468, 'security': 469, 'interviews': 470, 'japan': 471, 'happen': 472, 'quiz': 473, 'probably': 474, 'david': 475, 'didnt': 476, 'songs': 477, 'tour': 478, 'study': 479, 'israeli': 480, 'literally': 481, 'insanely': 482, 'israel': 483, 'strike': 484, 'posts': 485, 'might': 486, 'google': 487, 'ex': 488, 'facebook': 489, 'holiday': 490, 'celebrities': 491, 'wont': 492, 'justin': 493, 'released': 494, 'missing': 495, 'cut': 496, 'play': 497, 'america': 498, 'body': 499, 'damn': 500, 'fan': 501, 'michael': 502, 'phone': 503, 'definitely': 504, 'birth': 505, 'end': 506, 'team': 507, 'senate': 508, 'storm': 509, 'today': 510, 'cant': 511, 'story': 512, 'month': 513, 'power': 514, 'media': 515, 'another': 516, 'under': 517, 'color': 518, 'human': 519, 'money': 520, 'meet': 521, 'e': 522, 'run': 523, 'round': 524, '28': 525, 'beauty': 526, 'lady': 527, 'workers': 528, 'public': 529, 'explosion': 530, 'them': 531, 'determine': 532, 'identify': 533, 'guilty': 534, 'l': 535, 'history': 536, 'animals': 537, 'free': 538, 'buy': 539, 'honest': 540, 'race': 541, 'loss': 542, 'anti': 543, 'protest': 544, 'weird': 545, 'names': 546, 'national': 547, 'f': 548, 'aged': 549, 'flu': 550, 'couple': 551, 'jobs': 552, 'recipes': 553, 'important': 554, 'between': 555, 'pop': 556, 'horror': 557, 'child': 558, 'prime': 559, 'winter': 560, 'film': 561, 'match': 562, 'finds': 563, 'drug': 564, 'crisis': 565, 'charged': 566, 'gaza': 567, 'fucking': 568, 'couples': 569, 'great': 570, 'keep': 571, 'quotes': 572, 'university': 573, 'union': 574, 'europe': 575, 'small': 576, 'dad': 577, 'start': 578, 'economy': 579, 'children': 580, 'cats': 581, 'close': 582, 'mexico': 583, 'bad': 584, 'b': 585, 'night': 586, 'read': 587, 'marriage': 588, 'post': 589, 'parliament': 590, 'held': 591, 'killing': 592, 'strikes': 593, '2009': 594, 'troops': 595, 'heart': 596, 'funny': 597, 'judge': 598, 'reports': 599, 'bombing': 600, 'early': 601, 'service': 602, 'job': 603, 'mom': 604, 'different': 605, 'attacks': 606, 'call': 607, 'lost': 608, 'students': 609, 'accused': 610, 'officials': 611, 'launch': 612, 'rights': 613, 'hate': 614, 'gets': 615, '2010': 616, 'prison': 617, 'secret': 618, 'completely': 619, 'gives': 620, 'train': 621, 'sales': 622, 'mets': 623, 'profit': 624, 'protests': 625, 'very': 626, 'me': 627, 'think': 628, 'iconic': 629, 'student': 630, 'told': 631, 'future': 632, 'gas': 633, 'taliban': 634, 'begins': 635, 'yourself': 636, 'asked': 637, 'answer': 638, 'part': 639, 'official': 640, 'major': 641, 'pay': 642, 'return': 643, 'san': 644, 'seen': 645, 'famous': 646, 'classic': 647, 'where': 648, 'even': 649, 'getting': 650, 'mind': 651, 'americans': 652, 'online': 653, 'de': 654, 'german': 655, 'japanese': 656, 'al': 657, '50': 658, 'leaves': 659, 'art': 660, 'hard': 661, 'jenner': 662, 'move': 663, 'personality': 664, 'o': 665, 'thrones': 666, 'wants': 667, 'debate': 668, 'law': 669, 'seven': 670, 'airport': 671, 'governor': 672, 'general': 673, 'hurricane': 674, 'victory': 675, 'cuts': 676, 'opposition': 677, 'animal': 678, 'cover': 679, 'away': 680, 'ice': 681, 'title': 682, 'lead': 683, 'pm': 684, 'awesome': 685, 'thoughts': 686, 'costumes': 687, 'dating': 688, 'street': 689, 'apple': 690, 'budget': 691, 'funniest': 692, 'becomes': 693, 'bus': 694, 'boy': 695, 'behind': 696, 'pretty': 697, 'rock': 698, 'support': 699, 'put': 700, 'matches': 701, 'reveal': 702, 'sum': 703, 'valentines': 704, 'leave': 705, 'ends': 706, 'tax': 707, 'taste': 708, 'j': 709, 'stories': 710, 'because': 711, 'biggest': 712, 'eating': 713, 'called': 714, 'happens': 715, 'violence': 716, 'political': 717, 'forces': 718, 'using': 719, 'drunk': 720, 'doing': 721, 'amy': 722, 'pizza': 723, 'place': 724, 'shooting': 725, 'rise': 726, 'zimbabwe': 727, 'kardashian': 728, 'iphone': 729, 'science': 730, 'costume': 731, 'facts': 732, 'moment': 733, '33': 734, 'fight': 735, 'donald': 736, 'hogwarts': 737, 'whos': 738, 'search': 739, 'church': 740, 'town': 741, 'eight': 742, 'seeks': 743, 'belong': 744, '32': 745, 'album': 746, 'lives': 747, 'wrong': 748, 'having': 749, 'there': 750, 'ask': 751, 'absolutely': 752, 'kind': 753, 'premier': 754, 'northern': 755, 'third': 756, 'd': 757, 'building': 758, 'trade': 759, 'resigns': 760, 'awkward': 761, 'depression': 762, 'insane': 763, 'shit': 764, 'advice': 765, 'goals': 766, 'used': 767, 'ideas': 768, 'turkey': 769, 'buzzfeed': 770, 'texts': 771, 'african': 772, 'incredibly': 773, 'p': 774, 'youtube': 775, 'army': 776, 'care': 777, 'water': 778, 'nine': 779, 'supreme': 780, 'prices': 781, 'jet': 782, 'africa': 783, 'prix': 784, 'eu': 785, 'fashion': 786, 'korean': 787, '00s': 788, 'social': 789, 'soldiers': 790, 'baseball': 791, 'southern': 792, 'web': 793, 'release': 794, 'taiwan': 795, 'taking': 796, 'looking': 797, 'far': 798, 'cool': 799, 'won': 800, 'trump': 801, 'emergency': 802, 'low': 803, 'according': 804, 'head': 805, 'campaign': 806, 'ship': 807, 'investigation': 808, 'championship': 809, 'accident': 810, 'trailer': 811, 'adele': 812, 'tips': 813, 'kid': 814, 'level': 815, 'kim': 816, 'mean': 817, 'worlds': 818, 'pope': 819, 'reaches': 820, 'quarter': 821, 'rocket': 822, 'reported': 823, 'suspected': 824, 'stimulus': 825, 'charges': 826, 'fraud': 827, 'cancer': 828, 'doctor': 829, 'green': 830, 'secrets': 831, 'wear': 832, 'italian': 833, 'together': 834, 'ready': 835, 'must': 836, 'style': 837, 'thousands': 838, 'challenge': 839, 'company': 840, 'response': 841, 'chile': 842, 'washington': 843, 'helicopter': 844, 'financial': 845, 'greatest': 846, 'airlines': 847, 'awards': 848, 'tiny': 849, 'speech': 850, 'tom': 851, 'visit': 852, 'gave': 853, 'incredible': 854, 'running': 855, 'clinton': 856, 'opens': 857, 'east': 858, 'sri': 859, 'amid': 860, 'microsoft': 861, 'knows': 862, 'always': 863, 'model': 864, 'arent': 865, 'cheese': 866, 'stage': 867, 'actor': 868, 'tattoos': 869, 'jennifer': 870, 'sea': 871, 'artist': 872, 'goes': 873, 'drop': 874, 'mexican': 875, 'battle': 876, 'continues': 877, 'yankees': 878, 'inquiry': 879, 'shut': 880, 'episode': 881, 'scotland': 882, 'places': 883, 'britain': 884, 'picture': 885, 'turn': 886, 'teen': 887, 'english': 888, 'cards': 889, 'share': 890, 'days': 891, 'role': 892, 'blast': 893, 'save': 894, 'results': 895, 'site': 896, 'italy': 897, 'passes': 898, 'somali': 899, 'illinois': 900, 'shuttle': 901, 'son': 902, 'watching': 903, 'trying': 904, 'paris': 905, 'leaders': 906, 'charts': 907, 'ryan': 908, 'chris': 909, 'healthy': 910, 'believe': 911, 'each': 912, 'wikipedia': 913, 'control': 914, '2005': 915, 'him': 916, 'terror': 917, 'energy': 918, 'offers': 919, 'suspect': 920, 'warns': 921, 'drake': 922, 'looked': 923, 'badass': 924, 'possible': 925, 'gorgeous': 926, 'took': 927, 'hilariously': 928, 'late': 929, 'young': 930, 'action': 931, 'rebels': 932, 'industry': 933, 'officer': 934, 'senator': 935, 'bomber': 936, 'bid': 937, 'bankruptcy': 938, 'golden': 939, 'feels': 940, 'kanye': 941, 'bff': 942, 'james': 943, 'williams': 944, 'genius': 945, 'gilmore': 946, 'adult': 947, 'random': 948, 'moves': 949, 're': 950, 'player': 951, 'despite': 952, 'pirates': 953, 'germany': 954, 'market': 955, 'lovers': 956, 'desserts': 957, 'slightly': 958, 'then': 959, 'failed': 960, 'fuck': 961, 'earth': 962, 'non': 963, 'cold': 964, 'stay': 965, 'crossword': 966, 'foods': 967, 'send': 968, 'fails': 969, 'line': 970, 'inside': 971, 'wish': 972, 'past': 973, 'bring': 974, 'stars': 975, 'channel': 976, 'brazilian': 977, 'scientists': 978, 'queen': 979, 'jones': 980, 'ireland': 981, 'threat': 982, 'bombings': 983, 'magnitude': 984, 'female': 985, 'candy': 986, 'cutest': 987, 'sale': 988, 'anxiety': 989, 'posters': 990, 'lose': 991, 'married': 992, 'k': 993, '500': 994, 'nasa': 995, 'fast': 996, 'cricket': 997, 'plant': 998, 'iranian': 999, 'order': 1000, 'fake': 1001, 'problems': 1002, 'chill': 1003, 'short': 1004, 'hamilton': 1005, 'learned': 1006, 'living': 1007, 'full': 1008, 'issues': 1009, 'business': 1010, 'princess': 1011, 'worth': 1012, 'thai': 1013, 'interview': 1014, 'returns': 1015, 'across': 1016, 'deaths': 1017, 'due': 1018, 'arrest': 1019, 'indonesia': 1020, 'recession': 1021, 'defeat': 1022, 'federal': 1023, 'hear': 1024, 'finally': 1025, 'gift': 1026, 'hotel': 1027, 'reveals': 1028, 'sense': 1029, 'bieber': 1030, 'talking': 1031, 'houses': 1032, 'named': 1033, 'george': 1034, '0': 1035, 'sets': 1036, 'board': 1037, 'station': 1038, 'pakistani': 1039, 'usa': 1040, 'banks': 1041, 'hospital': 1042, 'climate': 1043, 'votes': 1044, 'knicks': 1045, 'toll': 1046, 'loves': 1047, 'im': 1048, 'ridiculously': 1049, '2006': 1050, 'crush': 1051, 'bbc': 1052, '34': 1053, 'prince': 1054, 'toronto': 1055, 'jersey': 1056, 'rescue': 1057, 'poll': 1058, 'beat': 1059, 'philippines': 1060, 'victims': 1061, 'releases': 1062, 'coach': 1063, 'term': 1064, 'following': 1065, 'offer': 1066, 'scientology': 1067, 'brazil': 1068, 'island': 1069, 'economic': 1070, 'continue': 1071, 'swine': 1072, 'secretary': 1073, 'injures': 1074, 'madoff': 1075, 'awakens': 1076, 'hacks': 1077, 'netflix': 1078, 'relate': 1079, 'done': 1080, 'dinner': 1081, 'ad': 1082, 'falls': 1083, 'happy': 1084, 'gender': 1085, 'r': 1086, 'met': 1087, 'spanish': 1088, 'drink': 1089, 'sexual': 1090, 'come': 1091, 'wife': 1092, 'radio': 1093, 'kylie': 1094, 'ten': 1095, 'list': 1096, 'turns': 1097, 'ahead': 1098, 'spain': 1099, 'loses': 1100, 'missile': 1101, 'left': 1102, 'type': 1103, 'immediately': 1104, 'santa': 1105, 'theyre': 1106, 'players': 1107, 'rose': 1108, 'comes': 1109, 'boyfriend': 1110, 'version': 1111, 'band': 1112, 'crazy': 1113, 'britney': 1114, 'reality': 1115, 'hollywood': 1116, 'porn': 1117, 'cry': 1118, 'break': 1119, 'data': 1120, 'shopping': 1121, 'controversial': 1122, 'lets': 1123, 't': 1124, 'special': 1125, 'since': 1126, 'tropical': 1127, 'moon': 1128, 'debt': 1129, 'fuel': 1130, 'defense': 1131, 'agree': 1132, 'council': 1133, 'foreign': 1134, 'conference': 1135, 'bird': 1136, 'protesters': 1137, 'dozens': 1138, 'singer': 1139, 'heartbreaking': 1140, 'mother': 1141, 'capture': 1142, 'changing': 1143, 'member': 1144, 'eye': 1145, 'fear': 1146, 'musical': 1147, 'videos': 1148, 'bowl': 1149, '35': 1150, 'sharing': 1151, 'trans': 1152, 'rugby': 1153, 'mayor': 1154, 'breaks': 1155, 'medical': 1156, 'shares': 1157, 'wall': 1158, 'companies': 1159, 'approves': 1160, 'reach': 1161, 'threatens': 1162, 'markets': 1163, 'stunning': 1164, 'powerful': 1165, 'almost': 1166, 'hearing': 1167, 'slow': 1168, 'ads': 1169, 'whole': 1170, 'chocolate': 1171, 'reminder': 1172, 'blow': 1173, 'king': 1174, 'pick': 1175, 'working': 1176, 'add': 1177, 'size': 1178, 'global': 1179, 'december': 1180, 'loud': 1181, 'alone': 1182, 'once': 1183, 'eyes': 1184, 'helped': 1185, 'scottish': 1186, 'dispute': 1187, 'abuse': 1188, 'co': 1189, 'basketball': 1190, 'less': 1191, 'wales': 1192, 'irish': 1193, 'journalist': 1194, 'justice': 1195, 'director': 1196, 'park': 1197, 'system': 1198, 'seek': 1199, 'discovery': 1200, 'border': 1201, 'central': 1202, 'hold': 1203, 'satellite': 1204, 'baghdad': 1205, 'quote': 1206, 'country': 1207, 'sexy': 1208, 'oscar': 1209, 'dress': 1210, 'hardest': 1211, 'theres': 1212, 'mumbai': 1213, 'appreciate': 1214, 'seriously': 1215, 'scream': 1216, 'any': 1217, 'pumpkin': 1218, 'spears': 1219, 'sell': 1220, 'miss': 1221, 'travel': 1222, 'policy': 1223, 'host': 1224, 'tries': 1225, 'sees': 1226, 'lanka': 1227, 'peace': 1228, 'executive': 1229, 'qaeda': 1230, 'rejects': 1231, 'hunger': 1232, 'taken': 1233, 'direction': 1234, 'minutes': 1235, 'wore': 1236, 'grew': 1237, 'birthday': 1238, 'straight': 1239, 'knew': 1240, 'friday': 1241, 'parks': 1242, 'something': 1243, 'treats': 1244, 'forever': 1245, 'asian': 1246, 'actual': 1247, 'cost': 1248, 'become': 1249, 'stock': 1250, 'summer': 1251, 'canadians': 1252, '40': 1253, 'childhood': 1254, 'several': 1255, 'files': 1256, 'boston': 1257, 'olympic': 1258, 'epic': 1259, 'network': 1260, 'virginia': 1261, 'mission': 1262, 'mine': 1263, 'cabinet': 1264, 'center': 1265, 'buffalo': 1266, 'road': 1267, 'large': 1268, 'denies': 1269, 'freed': 1270, 'jailed': 1271, 'gunman': 1272, 'georgia': 1273, 'coffee': 1274, 'class': 1275, 'lessons': 1276, 'finals': 1277, 'popular': 1278, 'serious': 1279, 'jokes': 1280, 'jimmy': 1281, 'magazine': 1282, 'self': 1283, 'push': 1284, 'gifs': 1285, 'jackson': 1286, 'author': 1287, 'award': 1288, 'mars': 1289, 'holds': 1290, 'starts': 1291, 'champions': 1292, 'michigan': 1293, 'congress': 1294, 'nato': 1295, 'fund': 1296, 'decline': 1297, 'raises': 1298, 'nations': 1299, 'sudan': 1300, 'shoot': 1301, 'princesses': 1302, 'bunch': 1303, 'account': 1304, 'memes': 1305, 'hell': 1306, 'reporter': 1307, 'weekend': 1308, 'squad': 1309, 'minute': 1310, 'truths': 1311, 'selena': 1312, 'store': 1313, 'johnson': 1314, 'let': 1315, 'safety': 1316, 'needed': 1317, 'others': 1318, 'wanted': 1319, 'banned': 1320, 'celebs': 1321, 'surprise': 1322, 'listen': 1323, 'brown': 1324, 'magical': 1325, 'announced': 1326, 'dance': 1327, 'credit': 1328, 'h': 1329, 'track': 1330, 'winning': 1331, 'reading': 1332, 'officers': 1333, 'driver': 1334, 'claim': 1335, 'arms': 1336, 'price': 1337, 'delay': 1338, 'strong': 1339, 'flooding': 1340, 'software': 1341, 'orders': 1342, 'formula': 1343, 'somalia': 1344, 'river': 1345, 'reaction': 1346, 'entire': 1347, 'mark': 1348, 'ultimate': 1349, 'chicken': 1350, 'images': 1351, 'favourite': 1352, 'capital': 1353, 'queens': 1354, 'simple': 1355, 'given': 1356, 'womens': 1357, 'wearing': 1358, 'sports': 1359, 'admits': 1360, 'changed': 1361, 'polish': 1362, 'meeting': 1363, 'mental': 1364, 'shop': 1365, 'light': 1366, 'bodies': 1367, 'pro': 1368, 'giant': 1369, 'clean': 1370, 'revealed': 1371, 'alleged': 1372, 'half': 1373, 'visits': 1374, 'review': 1375, 'egyptian': 1376, 'increase': 1377, 'clash': 1378, 'paul': 1379, 'st': 1380, 'turkish': 1381, 'saudi': 1382, 'soccer': 1383, 'leads': 1384, 'soldier': 1385, 'convicted': 1386, 'arrests': 1387, 'journalists': 1388, 'palestinian': 1389, 'program': 1390, 'egypt': 1391, 'choose': 1392, 'dreams': 1393, 'coming': 1394, 'francisco': 1395, 'draw': 1396, 'lesbian': 1397, 'experience': 1398, 'diy': 1399, 'terrible': 1400, 'ball': 1401, 'gomez': 1402, 'room': 1403, 'killer': 1404, 'kardashians': 1405, 'sent': 1406, 'nfl': 1407, 'truly': 1408, 'works': 1409, 'discovered': 1410, 'daughter': 1411, 'number': 1412, 'reason': 1413, 'spot': 1414, 'chicago': 1415, 'rebel': 1416, 'bar': 1417, 'stand': 1418, 'gun': 1419, 'changes': 1420, 'mass': 1421, 'firm': 1422, 'sentenced': 1423, 'blasts': 1424, 'bailout': 1425, 'corruption': 1426, 'declares': 1427, 'appeal': 1428, 'magic': 1429, 'crimes': 1430, 'hand': 1431, 'wtf': 1432, 'cake': 1433, 'word': 1434, 'lot': 1435, 'x': 1436, 'letter': 1437, 'breakfast': 1438, 'inspired': 1439, 'los': 1440, 'tattoo': 1441, 'hope': 1442, 'bride': 1443, 'hockey': 1444, 'films': 1445, 'side': 1446, 'thinks': 1447, 'holidays': 1448, 'absolute': 1449, 'struggle': 1450, 'adeles': 1451, 'television': 1452, 'image': 1453, 'laws': 1454, 'disaster': 1455, 'growing': 1456, 'charge': 1457, 'schools': 1458, 'months': 1459, 'passenger': 1460, 'middle': 1461, 'rate': 1462, 'details': 1463, 'likely': 1464, 'stadium': 1465, 'eastern': 1466, 'hundreds': 1467, 'administration': 1468, 'ruling': 1469, 'summit': 1470, 'indonesian': 1471, '200': 1472, 'detroit': 1473, 'sydney': 1474, 'rises': 1475, 'brother': 1476, 'snow': 1477, 'dinners': 1478, 'bollywood': 1479, 'recognize': 1480, 'indias': 1481, 'actors': 1482, '42': 1483, 'pass': 1484, 'tinder': 1485, 'winner': 1486, 'officially': 1487, 'lawrence': 1488, 'deep': 1489, 'angeles': 1490, 'ocean': 1491, 'god': 1492, 'key': 1493, 'doesnt': 1494, 'summed': 1495, 'v': 1496, 'choice': 1497, 'card': 1498, 'legal': 1499, '39': 1500, 'jack': 1501, 'raid': 1502, 'text': 1503, 'club': 1504, 'ellen': 1505, 'tonight': 1506, 'teens': 1507, 'tells': 1508, 'scare': 1509, 'dollar': 1510, 'carolina': 1511, 'civilians': 1512, 'caught': 1513, 'w': 1514, 'rangers': 1515, 'scandal': 1516, 'kingdom': 1517, 'computer': 1518, 'crew': 1519, 'fox': 1520, 'rape': 1521, 'receives': 1522, 'fears': 1523, 'labor': 1524, 'fifa': 1525, 'auto': 1526, 'landing': 1527, 'haiti': 1528, 'expected': 1529, 'causes': 1530, 'prepares': 1531, 'fires': 1532, 'gunmen': 1533, 'begin': 1534, 'opening': 1535, 'jessica': 1536, 'agency': 1537, 'taught': 1538, 'items': 1539, 'tired': 1540, 'lover': 1541, 'viral': 1542, 'thought': 1543, 'selfie': 1544, 'artists': 1545, 'motivational': 1546, 'cook': 1547, 'field': 1548, 'snapchat': 1549, 'costs': 1550, 'solve': 1551, 'facing': 1552, 'pet': 1553, 'bridge': 1554, 'point': 1555, 'press': 1556, 'sleep': 1557, 'beer': 1558, 'emo': 1559, 'please': 1560, 'act': 1561, 'faith': 1562, '37': 1563, 'democratic': 1564, 'rather': 1565, 'carpet': 1566, 'hero': 1567, 'fourth': 1568, 'voice': 1569, 'steve': 1570, 'members': 1571, 'spending': 1572, 'rally': 1573, 'museum': 1574, 'crime': 1575, 'republic': 1576, 'local': 1577, 'base': 1578, 'owner': 1579, '90': 1580, 'massive': 1581, 'until': 1582, 'mobile': 1583, 'navy': 1584, 'virus': 1585, 'drops': 1586, 'greek': 1587, 'cargo': 1588, 'taipei': 1589, 'myanmar': 1590, 'democrats': 1591, 'tsunami': 1592, 'ties': 1593, 'romantic': 1594, 'lines': 1595, 'teacher': 1596, 'bond': 1597, 'underrated': 1598, 'career': 1599, 'schumer': 1600, 'drinking': 1601, 'terrifying': 1602, 'male': 1603, 'figure': 1604, 'mcdonalds': 1605, 'follow': 1606, 'lazy': 1607, 'playing': 1608, 'sister': 1609, 'spice': 1610, 'scott': 1611, 'meets': 1612, 'speak': 1613, 'soul': 1614, 'blue': 1615, '36': 1616, 'dancing': 1617, 'join': 1618, 'amazon': 1619, 'basically': 1620, 'dressed': 1621, 'kendall': 1622, 'natural': 1623, 'themed': 1624, 'gordon': 1625, 'finish': 1626, 'warning': 1627, 'brand': 1628, 'double': 1629, 'morning': 1630, 'helps': 1631, 'dr': 1632, '45': 1633, 'gold': 1634, 'captures': 1635, 'steps': 1636, 'wrote': 1637, 'raise': 1638, 'asks': 1639, 'drugs': 1640, 'latest': 1641, 'pieces': 1642, 'lawsuit': 1643, 'weapons': 1644, 'thailand': 1645, 'enters': 1646, '2011': 1647, 'access': 1648, 'relief': 1649, 'barack': 1650, 'confirmed': 1651, 'guinea': 1652, 'beats': 1653, 'euro': 1654, 'utah': 1655, 'unveils': 1656, 'airways': 1657, 'fiji': 1658, 'clashes': 1659, 'research': 1660, 'investors': 1661, 'sue': 1662, 'brain': 1663, 'died': 1664, 'ie': 1665, 'chance': 1666, 'scenes': 1667, 'accidentally': 1668, 'slayed': 1669, 'warm': 1670, 'truth': 1671, 'fired': 1672, 'brilliant': 1673, 'sick': 1674, 'models': 1675, 'shia': 1676, 'emoji': 1677, 'cookies': 1678, 'proved': 1679, 'ass': 1680, 'tweet': 1681, 'comments': 1682, 'globes': 1683, 'step': 1684, 'colors': 1685, 'finding': 1686, 'themselves': 1687, 'beyond': 1688, 'toys': 1689, 'period': 1690, 'create': 1691, 'bake': 1692, 'boys': 1693, 'kitchen': 1694, 'puppy': 1695, 'wild': 1696, '60': 1697, 'shes': 1698, 'deserve': 1699, 'tough': 1700, 'dream': 1701, 'users': 1702, 'beach': 1703, 'woods': 1704, 'march': 1705, 'cause': 1706, 'outside': 1707, 'remains': 1708, 'collapse': 1709, 'arizona': 1710, 'project': 1711, 'alert': 1712, 'threats': 1713, 'romanian': 1714, 'focus': 1715, 'greece': 1716, 'gippsland': 1717, 'congo': 1718, 'aircraft': 1719, 'heavy': 1720, 'stake': 1721, 'nigeria': 1722, 'gulf': 1723, 'rising': 1724, 'outbreak': 1725, 'funds': 1726, 'agrees': 1727, 'probe': 1728, 'emmys': 1729, 'bowie': 1730, 'starbucks': 1731, 'exist': 1732, 'dicaprio': 1733, 'styles': 1734, 'marry': 1735, 'target': 1736, 'festival': 1737, 'theme': 1738, 'exactly': 1739, 'able': 1740, 'alive': 1741, 'humanity': 1742, 'meme': 1743, 'gaga': 1744, 'forgot': 1745, 'bizarre': 1746, 'alan': 1747, 'muslim': 1748, 'proof': 1749, 'waiting': 1750, '41': 1751, 'trick': 1752, 'yahoo': 1753, 'employees': 1754, 'partner': 1755, 'original': 1756, 'panel': 1757, 'ago': 1758, 'captured': 1759, 'missed': 1760, 'polls': 1761, 'wonderful': 1762, 'fun': 1763, 'himself': 1764, 'truck': 1765, 'adults': 1766, 'blood': 1767, 'unemployment': 1768, 'chemical': 1769, 'guard': 1770, 'concerns': 1771, 'anniversary': 1772, 'pennsylvania': 1773, 'suspended': 1774, 'remain': 1775, 'sony': 1776, 'plot': 1777, 'olympics': 1778, 'jail': 1779, 'among': 1780, 'cuba': 1781, 'militants': 1782, 'strip': 1783, 'appeals': 1784, 'losses': 1785, 'effort': 1786, 'manager': 1787, 'jets': 1788, 'pacific': 1789, 'excited': 1790, 'ed': 1791, 'hotline': 1792, 'comment': 1793, 'hello': 1794, 'tim': 1795, 'scene': 1796, 'leonardo': 1797, 'suit': 1798, 'tricks': 1799, 'soulmate': 1800, 'moms': 1801, 'plus': 1802, 'emma': 1803, 'calendar': 1804, 'toy': 1805, 'refugees': 1806, 'development': 1807, 'sweet': 1808, 'gif': 1809, 'rescued': 1810, 'weather': 1811, 'floods': 1812, 'impossible': 1813, 'swiss': 1814, 'chelsea': 1815, 'creepy': 1816, 'contract': 1817, 'culture': 1818, 'fifth': 1819, 'reynolds': 1820, 'whether': 1821, 'closed': 1822, '43': 1823, 'cocktails': 1824, 'design': 1825, 'stephen': 1826, 'founder': 1827, 'politicians': 1828, 'western': 1829, 'parts': 1830, 'losing': 1831, 'created': 1832, 'attempt': 1833, 'groups': 1834, 'bin': 1835, 'cash': 1836, 'christian': 1837, 'website': 1838, '80': 1839, 'fighting': 1840, 'tests': 1841, 'lawyer': 1842, 'islands': 1843, 'closes': 1844, 'uks': 1845, 'orleans': 1846, 'massachusetts': 1847, 'republican': 1848, 'centre': 1849, 'queensland': 1850, 'beijing': 1851, 'killings': 1852, 'ferry': 1853, 'chrysler': 1854, 'nascar': 1855, 'atlantic': 1856, 'explosions': 1857, 'merger': 1858, 'chilean': 1859, 'evacuated': 1860, 'growth': 1861, 'guantanamo': 1862, 'ukraine': 1863, 'hamas': 1864, 'elected': 1865, 'digital': 1866, 'advance': 1867, 'wikileaks': 1868, 'gains': 1869, 'agreement': 1870, 'youd': 1871, 'bling': 1872, 'mtv': 1873, 'comics': 1874, 'dick': 1875, 'private': 1876, 'bffs': 1877, 'edition': 1878, 'robert': 1879, 'nyc': 1880, 'forget': 1881, 'technology': 1882, 'cookie': 1883, 'isnt': 1884, 'zayn': 1885, 'grown': 1886, 'celeb': 1887, 'seconds': 1888, 'uses': 1889, 'husband': 1890, 'comic': 1891, 'bob': 1892, 'feelings': 1893, 'smith': 1894, 'vines': 1895, 'father': 1896, 'personal': 1897, 'leaving': 1898, 'plays': 1899, 'rodriguez': 1900, 'dude': 1901, 'tech': 1902, 'played': 1903, 'hands': 1904, 'sunshine': 1905, 'leaked': 1906, 'selling': 1907, 'bombs': 1908, 'traffic': 1909, 'buys': 1910, 'dutch': 1911, 'countries': 1912, 'collide': 1913, 'april': 1914, 'katrina': 1915, 'hopes': 1916, 'uefa': 1917, 'politician': 1918, 'largest': 1919, 'philippine': 1920, 'policeman': 1921, 'detained': 1922, 'increases': 1923, '08': 1924, 'confirms': 1925, 'suspects': 1926, 'defeats': 1927, 'total': 1928, 'civil': 1929, 'avoid': 1930, 'glorious': 1931, 'enough': 1932, 'nailed': 1933, 'blair': 1934, 'mail': 1935, 'boobs': 1936, 'performance': 1937, 'nominee': 1938, 'rihanna': 1939, 'percent': 1940, 'rachel': 1941, 'surgery': 1942, 'picks': 1943, 'driving': 1944, 'creative': 1945, 'penis': 1946, '38': 1947, 'message': 1948, 'language': 1949, 'simpsons': 1950, 'winners': 1951, 'abortion': 1952, 'weirdest': 1953, 'eve': 1954, 'difference': 1955, 'ridiculous': 1956, 'puzzle': 1957, 'already': 1958, 'mac': 1959, 'singing': 1960, 'weeks': 1961, 'fail': 1962, 'gigi': 1963, 'hes': 1964, 'also': 1965, 'rings': 1966, 'ii': 1967, 'january': 1968, 'candidates': 1969, 'dj': 1970, 'tea': 1971, 'proposal': 1972, 'celebrate': 1973, 'twenty': 1974, 'drive': 1975, 'titles': 1976, 'destroyed': 1977, 'rain': 1978, 'senior': 1979, 'decision': 1980, 'spend': 1981, 'parties': 1982, 'riots': 1983, 'minds': 1984, 'survivors': 1985, 'couldnt': 1986, 'pressure': 1987, 'those': 1988, 'brothers': 1989, 'funeral': 1990, 'receive': 1991, 'cities': 1992, 'leak': 1993, 'pilot': 1994, 'foundation': 1995, 'maker': 1996, 'eleven': 1997, 'considers': 1998, 'missouri': 1999, 'argentina': 2000}\n",
      "Dataset training size:  25600\n",
      "Dataset testing size:  6400\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Preprocessing' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ab93087386d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#creating testing and training datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Preprocessing' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Initialize instance of Preprocessing class\n",
    "dataset = Preprocessing(2000, 30)\n",
    "\n",
    "# Load dataset\n",
    "dataset.load_dataset()\n",
    "\n",
    "# Clean and tokenize dataset\n",
    "dataset.clean_data()\n",
    "dataset.tokenization()\n",
    "\n",
    "# Build vocab \n",
    "dataset.build_vocab()\n",
    "\n",
    "# Index words and pad headline sentences\n",
    "dataset.word_to_idx()\n",
    "dataset.padding_sentences()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "dataset.split_data()\n",
    "\n",
    "# Print data specifics:\n",
    "print(\"Dataset size: \", len(dataset.x))\n",
    "print(\"Dataset vocab: \", dataset.vocab)\n",
    "print(\"Dataset training size: \", len(dataset.y_train))\n",
    "print(\"Dataset testing size: \", len(dataset.x_test))\n",
    "\n",
    "# Initialize model\n",
    "seq_len = 30\n",
    "vocab_size = len(dataset.vocab)\n",
    "embedding_size = 64\n",
    "dropout = 0.25\n",
    "out_size = 32\n",
    "stride = 2\n",
    "filters = [2,3,4,5]\n",
    "model = HeadlineClassifier(seq_len, vocab_size, embedding_size, dropout, out_size, stride, filters)\n",
    "\n",
    "# Creating testing and training datasets\n",
    "train_set = DataClass(dataset['x_train'], dataset['y_train'])\n",
    "test_set = DataClass(dataset['x_test'], dataset['y_test'])\n",
    "\n",
    "# Train model\n",
    "learning_rate = 0.01\n",
    "batch_size = 12\n",
    "numOfIter = 100\n",
    "train(model, dataset, learning_rate, batch_size, train_set, test_set, numOfIter)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "991fbe55e20aa96684a77864442132e855c0c354e4a8fc77b9a964190396388d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
