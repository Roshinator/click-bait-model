{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "### TODO\n",
    "#### We are using a CNN, max pooling, and n-grams (a sequence of n words in a sentence) to construct this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imporant Documentation\n",
    "\n",
    "### PyTorch Resources:\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "#####    - https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058#:~:text=An%20N%2Dgram%20means%20a,3%2Dgram%20(trigram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tokenize data and build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, vocab_size, seq_len):\n",
    "        self.file_name = 'clickbait_data.csv'\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_padded = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_dataset(self):\n",
    "        # Load dataset from local directory \n",
    "        df = pd.read_csv(self.file_name)\n",
    "        \n",
    "        self.x = df['headline'].values\n",
    "        self.y = df['clickbait'].values\n",
    "\n",
    "    def clean_data(self):\n",
    "        # Clean data by removing all special characters. Convert words to lowercase\n",
    "        self.x = [re.sub(r'\\'','', headline).lower() for headline in self.x]\n",
    "        self.x = [re.sub(r'[^A-Za-z0-9]+',' ', headline).lower() for headline in self.x]\n",
    "\n",
    "    def tokenization(self):\n",
    "        # Tokenize all headlines\n",
    "        self.x = [nltk.tokenize.wordpunct_tokenize(headline) for headline in self.x]\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # Build vocab and return 'vocab_size' most common words\n",
    "        self.vocab = dict()\n",
    "\n",
    "        fdist = nltk.FreqDist()\n",
    "        for headline in self.x:\n",
    "            for word in headline:\n",
    "                fdist[word] += 1\n",
    "        \n",
    "        common_words = fdist.most_common(self.vocab_size)\n",
    "\n",
    "        for count, word in enumerate(common_words):\n",
    "            self.vocab[word[0]] = count+1\n",
    "    \n",
    "    def word_to_idx(self):\t\n",
    "        # Convert each token into index based representation \n",
    "        self.x_tokenized = list()\n",
    "        \n",
    "        for sentence in self.x:\n",
    "            temp = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocab.keys():\n",
    "                    temp.append(self.vocab[word])\n",
    "            self.x_tokenized.append(temp)\n",
    "    \n",
    "    def padding_sentences(self):\n",
    "        # Make all sentences equal length. \n",
    "        # If sentence is smaller than minimum length, pad it \n",
    "        idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), idx)\n",
    "            self.x_padded.append(sentence)\n",
    "            \n",
    "        self.x_padded = np.array(self.x_padded)\n",
    "\n",
    "    def split_data(self):\n",
    "        # Split data into training and testing sets\n",
    "        trnSize = int(len(self.x) * .8)\n",
    "        tstSize = int(len(self.x) * .2)\n",
    "        self.x_train = self.x[:trnSize]\n",
    "        self.y_train = self.y[:trnSize]\n",
    "        self.x_test = self.x[trnSize:]\n",
    "        self.y_test = self.y[trnSize:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadlineClassifier(torch.nn.ModuleList):\n",
    "    def __init__(self, params):\n",
    "        super(HeadlineClassifier, self).__init__()\n",
    "\n",
    "        self.seq_len = params.seq_len\n",
    "        self.num_words = params.num_words\n",
    "        self.embedding_size = params.embedding_size\n",
    "\n",
    "        # Dropout (used to reduce chance of overfitting by \"dropping\" units in neural net). Probability (p) set to 0.25\n",
    "        self.dropout = torch.nn.Dropout(0.25)\n",
    "\n",
    "        # Kernel sizes for CNN\n",
    "        self.k1 = 2\n",
    "        self.k2 = 3\n",
    "        self.k3 = 4\n",
    "        self.k4 = 5\n",
    "\n",
    "        # Output size for convolutions\n",
    "        self.out_size = params.out_size\n",
    "        # Number of strides for convolutions\n",
    "        self.stride = params.stride\n",
    "\n",
    "        # Embedding layer (lookup table that stores embeddings of a fixed dictionary and size)\n",
    "        self.embedding = torch.nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "\n",
    "        # Convolution layers (each is a 1D convolution over an input)\n",
    "        self.c1 = torch.nn.Conv1d(self.seq_len, self.out_size, self.k1, self.stride)\n",
    "        self.c2 = torch.nn.Conv1d(self.seq_len, self.out_size, self.k2, self.stride)\n",
    "        self.c3 = torch.nn.Conv1d(self.seq_len, self.out_size, self.k3, self.stride)\n",
    "        self.c4 = torch.nn.Conv1d(self.seq_len, self.out_size, self.k4, self.stride)\n",
    "            \n",
    "        # Max pooling layers (each applies 1D max pooling to input) \n",
    "        self.pool_1 = torch.nn.MaxPool1d(self.k1, self.stride)\n",
    "        self.pool_2 = torch.nn.MaxPool1d(self.k2, self.stride)\n",
    "        self.pool_3 = torch.nn.MaxPool1d(self.k3, self.stride)\n",
    "        self.pool_4 = torch.nn.MaxPool1d(self.k4, self.stride)\n",
    "\n",
    "        # Fully connected layer (applies linear transformation to data)\n",
    "        self.fc = torch.nn.Linear(self.in_features_fc(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  32000\n",
      "Dataset vocab:  {'to': 1, 'in': 2, 'the': 3, 'of': 4, 'you': 5, 'a': 6, 'for': 7, 'and': 8, 'on': 9, 'your': 10, 'is': 11, 'are': 12, 'that': 13, 'this': 14, 'with': 15, 'at': 16, 'will': 17, 'from': 18, 'new': 19, 'about': 20}\n",
      "Dataset training size:  25600\n",
      "Dataset testing size:  6400\n"
     ]
    }
   ],
   "source": [
    "# Initialize instance of Preprocessing class\n",
    "dataset = Preprocessing(20, 30)\n",
    "\n",
    "# Load dataset\n",
    "dataset.load_dataset()\n",
    "\n",
    "# Clean and tokenize dataset\n",
    "dataset.clean_data()\n",
    "dataset.tokenization()\n",
    "\n",
    "# Build vocab \n",
    "dataset.build_vocab()\n",
    "\n",
    "# Index words and pad headline sentences\n",
    "dataset.word_to_idx()\n",
    "dataset.padding_sentences()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "dataset.split_data()\n",
    "\n",
    "# Print data specifics:\n",
    "print(\"Dataset size: \", len(dataset.x))\n",
    "print(\"Dataset vocab: \", dataset.vocab)\n",
    "print(\"Dataset training size: \", len(dataset.y_train))\n",
    "print(\"Dataset testing size: \", len(dataset.x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "991fbe55e20aa96684a77864442132e855c0c354e4a8fc77b9a964190396388d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
