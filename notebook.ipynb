{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "### TODO\n",
    "#### We are using a CNN, max pooling, and n-grams (a sequence of n words in a sentence) to construct this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imporant Documentation\n",
    "\n",
    "### PyTorch Resources:\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html\n",
    "#####    - https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "#####    - https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058#:~:text=An%20N%2Dgram%20means%20a,3%2Dgram%20(trigram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Tokenize data and build vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "from pathlib import Path\n",
    "TRAINING_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, vocab_size, seq_len):\n",
    "        self.file_name = 'clickbait_data.csv'\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = None\n",
    "        self.x_tokenized = None\n",
    "        self.x_padded = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_dataset(self):\n",
    "        # Load dataset from local directory \n",
    "        df = pd.read_csv(self.file_name)\n",
    "        df = shuffle(df)\n",
    "        self.x = df['headline'].to_numpy()\n",
    "        self.y = df['clickbait'].to_numpy()\n",
    "\n",
    "    def clean_data(self):\n",
    "        # Clean data by removing all special characters. Convert words to lowercase\n",
    "        self.x = [re.sub(r'\\'','', headline).lower() for headline in self.x]\n",
    "        self.x = [re.sub(r'[^A-Za-z0-9]+',' ', headline).lower() for headline in self.x]\n",
    "\n",
    "    def tokenization(self):\n",
    "        # Tokenize all headlines\n",
    "        self.x = [nltk.tokenize.wordpunct_tokenize(headline) for headline in self.x]\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # Build vocab and return 'vocab_size' most common words\n",
    "        self.vocab = dict()\n",
    "\n",
    "        fdist = nltk.FreqDist()\n",
    "        for headline in self.x:\n",
    "            for word in headline:\n",
    "                fdist[word] += 1\n",
    "        \n",
    "        common_words = fdist.most_common(self.vocab_size)\n",
    "\n",
    "        for count, word in enumerate(common_words):\n",
    "            self.vocab[word[0]] = count+1\n",
    "    \n",
    "    def word_to_idx(self):\t\n",
    "        # Convert each token into index based representation \n",
    "        self.x_tokenized = list()\n",
    "        \n",
    "        for sentence in self.x:\n",
    "            temp = list()\n",
    "            for word in sentence:\n",
    "                if word in self.vocab.keys():\n",
    "                    temp.append(self.vocab[word])\n",
    "            self.x_tokenized.append(temp)\n",
    "    \n",
    "    def padding_sentences(self):\n",
    "        # Make all sentences equal length. \n",
    "        # If sentence is smaller than minimum length, pad it \n",
    "        idx = 0\n",
    "        self.x_padded = list()\n",
    "\n",
    "        for sentence in self.x_tokenized:\n",
    "            while len(sentence) < self.seq_len:\n",
    "                sentence.insert(len(sentence), idx)\n",
    "            while len(sentence) > self.seq_len:\n",
    "                print(\"Sentence is long, consider increasing the sequence length to prevent vocab dropping.\")\n",
    "                sentence.pop()\n",
    "            self.x_padded.append(sentence)\n",
    "            \n",
    "        self.x_padded = np.array(self.x_padded)\n",
    "\n",
    "    def split_data(self):\n",
    "        # Split data into training and testing sets\n",
    "        trnSize = int(len(self.x_padded) * .8)\n",
    "        tstSize = int(len(self.x_padded) * .2)\n",
    "        self.x_train = self.x_padded[:trnSize]\n",
    "        self.y_train = self.y[:trnSize]\n",
    "        self.x_test = self.x_padded[trnSize:]\n",
    "        self.y_test = self.y[trnSize:]\n",
    "    \n",
    "    def get_tokenized_string(self, sentence) -> list:\n",
    "        # Load dataset from local directory \n",
    "        string = sentence\n",
    "        string = re.sub(r'\\'','', string).lower()\n",
    "        string = re.sub(r'[^A-Za-z0-9]+',' ', string).lower()\n",
    "        string = nltk.tokenize.wordpunct_tokenize(string)\n",
    "        x_tokenized = list()\n",
    "        idx = 0\n",
    "        for word in string:\n",
    "            if word in self.vocab.keys():\n",
    "                x_tokenized.append(self.vocab[word])\n",
    "        while len(x_tokenized) < self.seq_len:\n",
    "            x_tokenized.insert(len(string), idx)\n",
    "        while len(x_tokenized) > self.seq_len:\n",
    "            print(\"Sentence is long, consider increasing the sequence length to prevent vocab dropping.\")\n",
    "            x_tokenized.pop()\n",
    "        x_padded = x_tokenized\n",
    "        return np.array([x_padded])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadlineClassifier(torch.nn.ModuleList):\n",
    "    def __init__(self, seq_len, num_words, embedding_size, dropout, out_size, stride, filters):\n",
    "        super(HeadlineClassifier, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.num_words = num_words\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Dropout (used to reduce chance of overfitting by \"dropping\" units in neural net). Probability (p) set to 0.25\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Kernel sizes for CNN\n",
    "        #pass it as params when we change the implmentation. Can keep it like this for now\n",
    "        self.filters = filters\n",
    "\n",
    "        # Output size for convolutions\n",
    "        self.out_size = out_size\n",
    "        # Number of strides for convolutions\n",
    "        self.stride = stride\n",
    "\n",
    "        # Embedding layer (lookup table that stores embeddings of a fixed dictionary and size)\n",
    "        self.embedding = torch.nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "\n",
    "        # Convolution layers (each is a 1D convolution over an input)\n",
    "        self.clayers = [torch.nn.Conv1d(self.seq_len, self.out_size, fltr, self.stride) for fltr in self.filters]\n",
    "        \n",
    "            \n",
    "        # Max pooling layers (each applies 1D max pooling to input) \n",
    "        self.poollayers = [torch.nn.MaxPool1d(fltr, self.stride) for fltr in self.filters]\n",
    "\n",
    "        # Fully connected layer (applies linear transformation to data)\n",
    "        self.fc = torch.nn.Linear(self.size_of_input(), 1)\n",
    "\n",
    "    \n",
    "    def size_of_input(self):\n",
    "        # Calculate input size for linear layer \n",
    "        pout = 0\n",
    "        for filter in self.filters:\n",
    "            cout = math.floor(((self.embedding_size - (filter - 1) - 1)/self.stride) + 1)\n",
    "            pout += math.floor(((cout - (filter - 1) - 1) / self.stride) + 1)\n",
    "        \n",
    "        return pout * self.out_size\n",
    "\n",
    "    '''Create forward pass of neural network. Consists of mainly ordering the different types of layers\n",
    "        Steps:\n",
    "            1. Pass tokenized words through embedding layer\n",
    "            2. Pass each embedded sentence through each convolution and max pooling layer\n",
    "            3. Reduce vector to linear layer\n",
    "    '''\n",
    "    def forward(self, input_X):\n",
    "        \n",
    "        x = self.embedding(input_X)\n",
    "\n",
    "        layerPasses = list()\n",
    "        for i in range(len(self.filters)):\n",
    "            temp = self.clayers[i](x)\n",
    "            #TODO: Why not sigmoid (there are a few common functions we can try)? Maybe this can be a question we answer\n",
    "            temp = torch.relu(temp)\n",
    "            temp = self.poollayers[i](temp)\n",
    "            layerPasses.append(temp)\n",
    "\n",
    "        unn = torch.cat(layerPasses, 2)\n",
    "        unn = unn.reshape(unn.size(0), -1)\n",
    "\n",
    "        output = torch.sigmoid(self.dropout(self.fc(unn))).squeeze()\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClass(torch.utils.data.Dataset):\n",
    "   def __init__(self, x, y):\n",
    "      self.x = x\n",
    "      self.y = y\n",
    "      \n",
    "   def __len__(self):\n",
    "      return len(self.x)\n",
    "      \n",
    "   def __getitem__(self, idx):\n",
    "      return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Returns: Training Loss, Testing Loss, Training Accuracy, Testing Accuracy\n",
    "def train(model, dataset, learning_rate, batch_size, train_data, test_data, numOfIter):\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size)\n",
    "\n",
    "    opt = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    trainLoss, testLoss, trainAccuracy, testAccuracy = [], [], [], []\n",
    "    for iter in range(numOfIter):\n",
    "        model.train()\n",
    "        train_predictions = []\n",
    "\n",
    "        for x,y in train_loader:\n",
    "            y = y.type(torch.FloatTensor) # Convert type of labels\n",
    "            \n",
    "            pred = model.forward(x) # Forward pass on input data \n",
    "\n",
    "            loss = torch.nn.functional.binary_cross_entropy(pred, y) # Measure the Binary Cross Entropy between the target and input probabilities\n",
    "\n",
    "            opt.zero_grad() # Set gradients to zero \n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            train_predictions += list(pred.detach().numpy()) # Append training predictions \n",
    "\n",
    "        # Evaluate model\n",
    "        model.eval() # Put model in evaluation mode\n",
    "        test_predictions = []\n",
    "        with torch.no_grad(): # Disable gradient calculation \n",
    "            for x, y in test_loader:\n",
    "                pred = model.forward(x)\n",
    "                test_predictions += list(pred.detach().numpy()) # Append test predictions\n",
    "\n",
    "        # Get model accuracy \n",
    "        train_acc = calc_acc(dataset.y_train, train_predictions)\n",
    "        test_acc = calc_acc(dataset.y_test, test_predictions)\n",
    "        train_loss = loss.item()\n",
    "        test_loss = loss.item()\n",
    "        \n",
    "        # Gather training and testing loss & accuracy\n",
    "        trainLoss.append(train_loss)\n",
    "        testLoss.append(test_loss)\n",
    "        trainAccuracy.append(train_acc)\n",
    "        testAccuracy.append(test_acc)\n",
    "        \n",
    "        # Print training and testing accuracy for each iteration\n",
    "        print(\"Iteration: %d, Loss: %.5f, Train Accuracy: %.5f\" % (iter+1, train_loss, train_acc))\n",
    "        print(\"Iteration: %d, Loss: %.5f, Test Accuracy: %.5f\" % (iter+1, test_loss, test_acc))\n",
    "    \n",
    "    return (trainLoss, testLoss, trainAccuracy, testAccuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of model \n",
    "def calc_acc(actual, predictions):\n",
    "    clickbait = 0\n",
    "    not_clickbait = 0\n",
    "\n",
    "    for true, pred in zip(actual, predictions):\n",
    "        if (pred >= .5) and (true == 1): # True positive (prediction is 'clickbait' and actual is 'clickbait')\n",
    "            clickbait += 1\n",
    "        elif (pred < .5) and (true == 0): # True negative (prediction is 'not clickbait' and actual is 'not clickbait')\n",
    "            not_clickbait += 1\n",
    "\n",
    "    # Return accuracy of model\n",
    "    return (clickbait + not_clickbait) / len(actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:  32000\n",
      "Dataset training size:  25600\n",
      "Dataset testing size:  6400\n"
     ]
    }
   ],
   "source": [
    "seq_len = 30\n",
    "# Initialize instance of Preprocessing class\n",
    "dataset = Preprocessing(100_000, seq_len)\n",
    "\n",
    "# Load dataset\n",
    "dataset.load_dataset()\n",
    "\n",
    "# Clean and tokenize dataset\n",
    "dataset.clean_data()\n",
    "dataset.tokenization()\n",
    "\n",
    "# Build vocab \n",
    "dataset.build_vocab()\n",
    "\n",
    "# Index words and pad headline sentences\n",
    "dataset.word_to_idx()\n",
    "dataset.padding_sentences()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "dataset.split_data()\n",
    "\n",
    "# Print data specifics:\n",
    "print(\"Dataset size: \", len(dataset.x))\n",
    "#print(\"Dataset vocab: \", dataset.vocab)\n",
    "print(\"Dataset training size: \", len(dataset.y_train))\n",
    "print(\"Dataset testing size: \", len(dataset.x_test))\n",
    "\n",
    "# Initialize model (TODO: optimize parameters)\n",
    "vocab_size = len(dataset.vocab)\n",
    "embedding_size = 64\n",
    "dropout = 0.25\n",
    "out_size = 40\n",
    "stride = 2\n",
    "filters = [2,3,4,5]\n",
    "model = HeadlineClassifier(seq_len, vocab_size, embedding_size, dropout, out_size, stride, filters)\n",
    "\n",
    "# Model save paths model\n",
    "save_path = Path(\"model-save-data/model.pth\")\n",
    "cpu_flag_path = Path(\"model-save-data/cpu\")\n",
    "gpu_flag_path = Path(\"model-save-data/gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING_MODE:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    if device == 'cuda':\n",
    "        print(\"--------------Found CUDA device. Training on GPU.--------------\")\n",
    "    else:\n",
    "        print(\"--------------Training on CPU--------------\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Creating testing and training datasets\n",
    "    train_set = DataClass(dataset.x_train, dataset.y_train)\n",
    "    test_set = DataClass(dataset.x_test, dataset.y_test)\n",
    "\n",
    "    # Train model\n",
    "    learning_rate = 0.01\n",
    "    batch_size = 12\n",
    "    numOfIter = 100\n",
    "    trainLoss, testLoss, trainAccuracy, testAccuracy = train(model, dataset, learning_rate, batch_size, train_set, test_set, numOfIter)\n",
    "\n",
    "    # Delete old save data\n",
    "    if save_path.exists():\n",
    "        save_path.unlink()\n",
    "    if cpu_flag_path.exists():\n",
    "        cpu_flag_path.unlink()\n",
    "    if gpu_flag_path.exists():\n",
    "        gpu_flag_path.unlink()\n",
    "\n",
    "    # Write save and training device marker\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    if device == 'cuda':\n",
    "        \n",
    "        Path(gpu_flag_path).touch()\n",
    "    else:\n",
    "        Path(cpu_flag_path).touch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numOfIter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-be655c71a329>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plot losses/accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumOfIter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tab:blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainAccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tab:green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'numOfIter' is not defined"
     ]
    }
   ],
   "source": [
    "#plot losses/accuracies\n",
    "x = list(range(1, numOfIter + 1))\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(x, trainLoss, label='Training Loss', color='tab:blue', linewidth=3)\n",
    "plt.plot(x, trainAccuracy, label='Training Accuracy', color='tab:green', linewidth=3)\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.legend(fancybox=True, framealpha=0.5, fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "plt.plot(x, testLoss, label='Testing Loss', color='tab:blue', linewidth=3)\n",
    "plt.plot(x, testAccuracy, label='Testing Accuracy', color='tab:green', linewidth=3)\n",
    "plt.title('Testing Loss and Accuracy')\n",
    "plt.legend(fancybox=True, framealpha=0.5, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeadlineClassifier(\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (embedding): Embedding(23328, 64, padding_idx=0)\n",
       "  (fc): Linear(in_features=2320, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_device = 'cuda' if Path(gpu_flag_path).exists() else 'cpu'\n",
    "my_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = HeadlineClassifier(seq_len, vocab_size, embedding_size, dropout, out_size, stride, filters)\n",
    "if save_device == 'cuda' and my_device == 'cpu':\n",
    "    model.load_state_dict(torch.load(save_path, map_location=my_device))\n",
    "elif save_device == 'cuda' and my_device == 'cuda':\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.to(my_device)\n",
    "elif save_device == 'cuda' and my_device == 'cuda':\n",
    "    model.load_state_dict(torch.load(save_path, map_location=\"cuda:0\"))\n",
    "    model.to(device)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    \n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLICKBAIT FOUND ( result = 0.60589075 )\n"
     ]
    }
   ],
   "source": [
    "#Run a prediction test\n",
    "def predict(string) -> float:\n",
    "    test_phrase = dataset.get_tokenized_string(string)\n",
    "    with torch.no_grad(): # Disable gradient calculation\n",
    "        pred = model(torch.tensor(test_phrase))\n",
    "        return pred.detach().numpy()\n",
    "\n",
    "prediction = predict(\"this big brain person oofed themselves\")\n",
    "if prediction > 0.5:\n",
    "    print(\"CLICKBAIT FOUND ( result =\", prediction, \")\")\n",
    "else:\n",
    "    print(\"Not clickbait ( result =\", prediction, \")\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "991fbe55e20aa96684a77864442132e855c0c354e4a8fc77b9a964190396388d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
